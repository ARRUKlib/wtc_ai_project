# === main.py ===
from fastapi import FastAPI, Request
from langchain.llms import LlamaCpp
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from modules.tools import ToolRouter
from modules.rag import get_context_from_openai_api

llm = LlamaCpp(
    model_path="./models/deepseek-r1.gguf",
    temperature=0.3,           # ğŸ‘ˆ à¸„à¸§à¸šà¸„à¸¸à¸¡à¸„à¸§à¸²à¸¡ "à¸«à¸¥à¸¸à¸”à¹‚à¸¥à¸"
    top_p=0.8,                 # ğŸ‘ˆ à¸„à¸§à¸²à¸¡à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢ (nucleus sampling)
    top_k=40,                  # ğŸ‘ˆ à¸à¸£à¸­à¸‡ token à¸—à¸µà¹ˆà¸¡à¸µà¹‚à¸­à¸à¸²à¸ªà¸™à¹‰à¸­à¸¢
    max_tokens=1024,           # ğŸ‘ˆ à¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¸‚à¸­à¸‡à¸„à¸³à¸•à¸­à¸š
    repeat_penalty=1.1,        # ğŸ‘ˆ à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸„à¸³à¸‹à¹‰à¸³
    n_batch=64,                # ğŸ‘ˆ batch size à¸‚à¸“à¸°à¸£à¸±à¸™
    n_gpu_layers=32,           # ğŸ‘ˆ à¹ƒà¸Šà¹‰ GPU à¹„à¸”à¹‰à¸–à¹‰à¸²à¸¡à¸µ (à¸ªà¸³à¸«à¸£à¸±à¸š RunPod)
    verbose=True
)


memory = ConversationBufferMemory()
chain = ConversationChain(llm=llm, memory=memory)

app = FastAPI()

@app.post("/ask")
async def ask_user(request: Request):
    body = await request.json()
    question = body.get("question", "")
    context = get_context_from_openai_api(question)
    response = chain.predict(input=f"Context: {context}\nQuestion: {question}")
    return {"answer": response}